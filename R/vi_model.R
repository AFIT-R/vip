#' Model-Based Variable Importance
#'
#' Compute model-based variable importance scores for the predictors in a model.
#' (This function is meant for internal use only.)
#'
#' @param object A fitted model object (e.g., a \code{"randomForest"} object).
#'
#' @param auc Logical indicating whether or not to compute the AUC-based
#' variable scores described in Janitza et al. (2012). Only available for
#' \code{\link[party]{cforest}} objects. See
#' \code{\link[party]{varimpAUC}} for details. Default is \code{FALSE}.
#'
#' @param type For xgboost models, the type of variable importance to return.
#' If \code{NULL} (the default), "Gain" is used. See
#' \code{\link[xgboost]{xgb.importance}} for details.
#'
#' @param ... Additional optional arguments.
#'
#' @return A tidy data frame (i.e., a \code{"tibble"} object) with two columns:
#' \code{Variable} and \code{Importance}. For \code{"glm"}-like object, an
#' additional column, called \code{Sign}, is also included which includes the
#' sign (i.e., POS/NEG) of the original coefficient.
#'
#' @details
#'
#' Computes model-based variable importance scores depending on the class of
#' \code{object}:
#'
#' \describe{
#'
#' \item{\code{\link[C50]{C5.0}}}{Variable importance is measured by determining
#' the percentage of training set samples that fall into all the terminal nodes
#' after the split. For example, the predictor in the first split automatically
#' has an importance measurement of 100 percent since all samples are affected
#' by this split. Other predictors may be used frequently in splits, but if the
#' terminal nodes cover only a handful of training set samples, the importance
#' scores may be close to zero. The same strategy is applied to rule-based
#' models and boosted versions of the model. The underlying function can also
#' return the number of times each predictor was involved in a split by using
#' the option \code{metric = "usage"}. See \code{\link[C50]{C5imp}} for
#' details.}
#'
#' \item{\code{\link[partykit]{cforest}}}{Variable importance is measured in a
#' way similar to those computed by \code{\link[randomForest]{importance}}.
#' Besides the standard version, a conditional version is available that
#' adjusts for correlations between predictor variables. If
#' \code{conditional = TRUE}, the importance of each variable is computed by
#' permuting within a grid defined by the predictors that are associated (with
#' 1 - \emph{p}-value greater than threshold) to the variable of interest. The
#' resulting variable importance score is conditional in the sense of beta
#' coefficients in regression models, but represents the effect of a variable in
#' both main effects and interactions. See Strobl et al. (2008) for details.
#' Note, however, that all random forest results are subject to random
#' variation. Thus, before interpreting the importance ranking, check whether
#' the same ranking is achieved with a different random seed - or otherwise
#' increase the number of trees ntree in \code{\link[partykit]{ctree_control}}.
#' Note that in the presence of missings in the predictor variables the
#' procedure described in Hapfelmeier et al. (2012) is performed. See
#' \code{\link[partykit]{varimp}} for details.}
#'
#' \item{\code{\link[earth]{earth}}}{The variable importance method for
#' \code{\link[earth]{earth}} objects first calculates the decrease in the GCV for each
#' subset relative to the previous subset during earthâ€™s pruning pass. (For
#' multiple response models, GCV's are calculated over all responses.) Then, for
#' each variable, it sums these decreases over all subsets that include the
#' variable. Finally, for ease of interpretation, the summed decreases are
#' scaled so the largest summed decrease is 100. Variables which cause larger
#' net decreases in the GCV are considered more important. See
#' \code{\link[earth]{evimp}} for details.}
#'
#' \item{\code{\link[gbm]{gbm}}}{Variable importance is computed according to
#' Friedman (2001). For tree-based methods, the approximate relative influence
#' of a variable \eqn{x_j} is \deqn{\widehat{J}_j^2 = \sum{\text{splits on } x_j}I_t^2}
#' where \eqn{I_t} is the empirical improvement by splitting on \eqn{x_j} at that point.
#' Friedman's extension to boosted models is to average the relative influence
#' of variable \eqn{x_j} across all the trees generated by the boosting algorithm.
#' See \code{\link[gbm]{summary.gbm}} for details.}
#'
#' \item{\code{\link[h2o]{H2OModel}}}{See \code{\link[h2o]{h2o.varimp}} or visit
#' \url{http://docs.h2o.ai/h2o/latest-stable/h2o-docs/variable-importance.html}
#' for details.}
#'
#' \item{\code{\link[stats]{lm}}}{In (generalized) linear models, variable
#' importance is based on the absolute value of the corresponding
#' \emph{t}-statistics. For such models, the sign of the original coefficient
#' is also returned.}
#'
#' \item{\code{\link[sparklyr::ml_feature_importances]{ml_model}}}{The Spark ML
#' library provides standard variable importance for tree-based methods (e.g.,
#' random forests). See \code{\link[sparklyr]{ml_feature_importances}} for
#' details.}
#'
#' \item{\code{\link[randomForest]{randomForest}}}{Random forests typically
#' provide two measures of variable importance. The first measure is computed
#' from permuting out-of-bag (OOB) data: for each tree, the prediction error on
#' the OOB portion of the data is recorded (error rate for classification and
#' MSE for regression). Then the same is done after permuting each predictor
#' variable. The difference between the two are then averaged over all trees in
#' the forest, and normalized by the standard deviation of the differences. If
#' the standard deviation of the differences is equal to 0 for a variable,
#' the division is not done (but the average is almost always equal to 0 in that
#' case).
#'
#' The second measure is the total decrease in node impurities from splitting on
#' the variable, averaged over all trees. For classification, the node impurity
#' is measured by the Gini index. For regression, it is measured by residual sum
#' of squares. See \code{\link[randomForest]{importance}} for details.}
#'
#' \item{\code{\link[party]{RandomForest}}}{Same approach described in
#' \code{\link[partykit]{cforest}} above. See \code{\link[party]{varimp}} and
#' \code{\link[party]{varimpAUC}} (if \code{auc = TRUE}) for details.}
#'
#' \item{\code{\link[ranger]{ranger}}}{Variable importance for
#' \code{\link[ranger]{ranger}} objects is computed in the usual way for random
#' forests. The approach used depends on the \code{importance} argument provided
#' in the initial call to \code{\link[ranger]{ranger}}. See
#' \code{\link[ranger]{importance}} for details.}
#'
#' \item{\code{\link[rpart]{rpart}}}{As stated in one of the \strong{rpart}
#' vignettes. A variable may appear in the tree many times, either as a primary
#' or a surrogate variable. An overall measure of variable importance is the sum
#' of the goodness of split measures for each split for which it was the primary
#' variable, plus "goodness" * (adjusted agreement) for all splits in which it
#' was a surrogate. Imagine two variables which were essentially duplicates of
#' each other; if we did not count surrogates, they would split the importance
#' with neither showing up as strongly as it should. See
#' \code{\link[rpart]{rpart}} for details.}
#'
#' \item{\code{\link[caret]{train}}}{Various model-specific and model-agnostic
#' approaches that depend on the learning algorithm employed in the original
#' call to \code{\link[caret]{train}}. See \code{\link[caret]{varImp}} for
#' details.}
#'
#' \item{\code{\link[xgboost]{xgb.Booster}}}{For linear models, the variable
#' importance is the absolute magnitude of the estimated coefficients. For that
#' reason, in order to obtain a meaningful ranking by importance for a linear
#' model, the features need to be on the same scale (which you also would want
#' to do when using either L1 or L2 regularization). Otherwise, the approach
#' described in Friedman (2001) for \code{\link[gbm]{gbm}}s is used. See
#' \code{\link[xgboost]{xgb.importance}} for details.}
#'
#' }
#'
#' @note Inspired by the \code{\link[caret]{varImp}} function.
#'
#' @rdname vi_model
#'
#' @export
vi_model <- function(object, ...) {
  UseMethod("vi_model")
}


#' @rdname vi_model
#'
#' @export
vi_model.default <- function(object, ...) {
  stop("Model-based variable importance scores are currently not available ",
       "for objects of class ", "\"", class(object), "\".")
}


#' @rdname vi_model
#'
#' @export
vi_model.C5.0 <- function(object, ...) {

  # Consruct model-based variable importance scores
  vis <- C50::C5imp(object, ...)
  tib <- tibble::tibble(
    "Variable" = rownames(vis),
    "Importance" = vis$Overall
  )

  # Add variable importance type attribute
  dot_pairlist <- substitute(...())
  attr(tib, "type") <- if ("metric" %in% names(dot_pairlist)) {
    eval(dot_pairlist[["metric"]], envir = parent.frame())
  } else {
    "usage"
  }

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.constparty <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- partykit::varimp(object, ...)
  tib <- tibble::tibble(
    "Variable" = names(vis),
    "Importance" = vis
  )

  # Add variable importance type attribute
  attr(tib, "type") <- "permutation"

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.earth <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- earth::evimp(object, trim = FALSE, ...)[, "gcv", drop = TRUE]
  tib <- tibble::tibble(
    "Variable" = names(vis),
    "Importance" = vis
  )
  tib$Variable <- gsub("-unused$", replacement = "", x = tib$Variable)

  # Add variable importance type attribute
  attr(tib, "type") <- "GCV"

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.gbm <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- summary(object, plotit = FALSE, order = TRUE, ...)
  tib <- tibble::tibble(
    "Variable" = vis$var,
    "Importance" = vis$rel.inf
  )

  # Add variable importance type attribute
  attr(tib, "type") <- "rel.inf"

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.H2OBinomialModel <- function(object, ...) {

  # Construct model-based variable importance scores
  tib <- tibble::as.tibble(h2o::h2o.varimp(object))
  if (object@algorithm == "glm") {
    names(tib) <- c("Variable", "Importance", "Sign")
    # FIXME: Extra row at the bottom?
  } else {
    tib <- tib[1L:2L]
    names(tib) <- c("Variable", "Importance")
  }

  # Add variable importance type attribute
  attr(tib, "type") <- "h2o"

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.H2OMultinomialModel <- function(object, ...) {

  # Construct model-based variable importance scores
  tib <- tibble::as.tibble(h2o::h2o.varimp(object))
  if (object@algorithm == "glm") {
    names(tib) <- c("Variable", "Importance", "Sign")
    # FIXME: Extra row at the bottom?
  } else {
    tib <- tib[1L:2L]
    names(tib) <- c("Variable", "Importance")
  }

  # Add variable importance type attribute
  attr(tib, "type") <- "h2o"

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.H2ORegressionModel <- function(object, ...) {

  # Construct model-based variable importance scores
  tib <- tibble::as.tibble(h2o::h2o.varimp(object))
  if (object@algorithm == "glm") {
    names(tib) <- c("Variable", "Importance", "Sign")
    # FIXME: Extra row at the bottom?
  } else {
    tib <- tib[1L:2L]
    names(tib) <- c("Variable", "Importance")
  }

  # Add variable importance type attribute
  attr(tib, "type") <- "h2o"

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.lm <- function(object, ...) {

  # Construct model-based variable importance scores
  coefs <- summary(object)$coefficients
  if (attr(object$terms, "intercept") == 1) {
    coefs <- coefs[-1L, , drop = FALSE]
  }
  tib <- tibble::tibble(
    "Variable" = rownames(coefs),
    "Importance" = abs(coefs[, "t value"]),
    "Sign" = ifelse(sign(coefs[, "Estimate"]) == 1, yes = "POS", no = "NEG")
  )

  # Add variable importance type attribute
  attr(tib, "type") <- "t-test"

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.ml_model_decision_tree_regression <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- sparklyr::ml_feature_importances(object, ...)
  names(vis) <- c("Variable", "Importance")
  type <- "spark_tree"
  tib <- tibble::as_tibble(vis)

  # Add variable importance type attribute
  attr(tib, "type") <- type

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.ml_model_decision_tree_classification <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- sparklyr::ml_feature_importances(object, ...)
  names(vis) <- c("Variable", "Importance")
  type <- "spark_tree"
  tib <- tibble::as_tibble(vis)

  # Add variable importance type attribute
  attr(tib, "type") <- type

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.ml_model_gbt_regression <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- sparklyr::ml_feature_importances(object, ...)
  names(vis) <- c("Variable", "Importance")
  type <- "spark_gbt"
  tib <- tibble::as_tibble(vis)

  # Add variable importance type attribute
  attr(tib, "type") <- type

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.ml_model_gbt_classification <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- sparklyr::ml_feature_importances(object, ...)
  names(vis) <- c("Variable", "Importance")
  type <- "spark_gbt"
  tib <- tibble::as_tibble(vis)

  # Add variable importance type attribute
  attr(tib, "type") <- type

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.ml_model_generalized_linear_regression <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- sparklyr::tidy(object, ...)[, c("term", "statistic")]
  if (vis$term[1L] == "(Intercept)") {
    vis <- vis[-1L, ]
  }
  vis$Sign <- ifelse(sign(vis$statistic) == 1, yes = "POS", no = "NEG")
  vis$statistic <- abs(vis$statistic)
  names(vis) <- c("Variable", "Importance", "Sign")
  type <- "spark_glm"
  tib <- tibble::as_tibble(vis)

  # Add variable importance type attribute
  attr(tib, "type") <- type

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.ml_model_linear_regression <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- sparklyr::tidy(object, ...)[, c("term", "statistic")]
  if (vis$term[1L] == "(Intercept)") {
    vis <- vis[-1L, ]
  }
  vis$Sign <- ifelse(sign(vis$statistic) == 1, yes = "POS", no = "NEG")
  vis$statistic <- abs(vis$statistic)
  names(vis) <- c("Variable", "Importance", "Sign")
  type <- "spark_lm"
  tib <- tibble::as_tibble(vis)

  # Add variable importance type attribute
  attr(tib, "type") <- type

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.ml_model_random_forest_regression <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- sparklyr::ml_feature_importances(object, ...)
  names(vis) <- c("Variable", "Importance")
  type <- "spark_rf"
  tib <- tibble::as_tibble(vis)

  # Add variable importance type attribute
  attr(tib, "type") <- type

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.ml_model_random_forest_classification <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- sparklyr::ml_feature_importances(object, ...)
  names(vis) <- c("Variable", "Importance")
  type <- "spark_rf"
  tib <- tibble::as_tibble(vis)

  # Add variable importance type attribute
  attr(tib, "type") <- type

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.randomForest <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- randomForest::importance(object, ...)
  type <- colnames(vis)[1L]
  if (dim(vis)[2L] == 0) {  # possible when importance = FALSE in RF call
    importance_scores <- object$importance
  }
  vis <- vis[, 1L, drop = TRUE]
  tib <- tibble::tibble(
    "Variable" = names(vis),
    "Importance" = vis
  )

  # Add variable importance type attribute
  attr(tib, "type") <- type

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.RandomForest <- function(object, auc = FALSE, ...) {

  # Construct model-based variable importance scores
  vis <- if (auc) {
    party::varimpAUC(object)  # rm ... for now
  } else {
    party::varimp(object)  # rm ... for now
  }
  tib <- tibble::tibble(
    "Variable" = names(vis),
    "Importance" = vis
  )

  # Add variable importance type attribute
  attr(tib, "type") <- if (auc) {
    "AUC"
  } else {
    "Permutation"
  }

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.ranger <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- ranger::importance(object)
  tib <- tibble::tibble(
    "Variable" = names(vis),
    "Importance" = vis
  )

  # Add variable importance type attribute
  attr(tib, "type") <- object$importance.mode

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.rpart <- function(object, ...) {

  # Construct model-based variable importance scores
  importance_scores <- object$variable.importance
  if (is.null(importance_scores)) {
    stop("Cannot extract variable importance scores from a tree with no ",
         "splits.", call. = FALSE)
  }
  feature_names <- names(importance_scores)

  # Place variable importance scores in a tibble (the first and second columns
  # should always be labelled "Variable" and "Importance", respectively)
  tib <- tibble::tibble(
    "Variable" = feature_names,
    "Importance" = importance_scores
  )

  # Add variable importance type attribute
  attr(tib, "type") <- "GoodnessOfSplit"

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.train <- function(object, ...) {

  # Construct model-based variable importance scores
  vis <- caret::varImp(object, ...)
  if (inherits(vis, "varImp.train")) {
    vis <- vis$importance
  }
  tib <- tibble::tibble(
    "Variable" = rownames(vis),
    "Importance" = vis$Overall
  )

  # Add variable importance type attribute
  attr(tib, "type") <- "caret"

  # Return results
  tib

}


#' @rdname vi_model
#'
#' @export
vi_model.xgb.Booster <- function(object, type = NULL, ...) {

  # Construct model-based variable importance scores
  imp_mat <- xgboost::xgb.importance(model = object, ...)
  if (is.null(type)) {
    type <- "Gain"
  } else {
    if (!(type %in% c("Gain", "Cover", "Frequency"))) {
      stop("Argument `type` must be one of \"Gain\", \"Cover\", or ",
           "\"Frequency\".", call. = FALSE)
    }
  }
  vis <- tibble::as.tibble(imp_mat)[, c("Feature", type)]
  tib <- tibble::tibble(
    "Variable" = vis$Feature,
    "Importance" = vis[[2L]]
  )

  # Add variable importance type attribute
  attr(tib, "type") <- type

  # Return results
  tib

}
