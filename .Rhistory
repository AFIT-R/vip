as.data.frame() %>%
tibble::rownames_to_column("Variable")
p2 <- ggplot(imp2, aes(x = reorder(Variable, IncNodePurity), y = IncNodePurity)) +
geom_col() +
coord_flip() +
xlab("") +
theme_light()
# Importance: partial dependence
p3 <- vip(trn.rf, use.partial = TRUE, pred.var = paste0("x.", 1:10), fill = "grey35") +
ylab("pdVarImp") +
theme_light()
# Display all three plots together
grid.arrange(p1, p2, p3, ncol = 3)
library(vip)
# Load required packages
library(magrittr)
library(randomForest)
library(vip)
# Fit a random forest
set.seed(102)
trn.rf <- randomForest(y ~ ., data = trn, importance = TRUE)
# Importance: mean decrease in accuracy
imp1 <- importance(trn.rf, type = 1) %>%
as.data.frame() %>%
tibble::rownames_to_column("Variable")
p1 <- ggplot(imp1, aes(x = reorder(Variable, `%IncMSE`), y = `%IncMSE`)) +
geom_col() +
coord_flip() +
xlab("") +
theme_light()
# Importance: mean decrease node impurity
imp2 <- importance(trn.rf, type = 2) %>%
as.data.frame() %>%
tibble::rownames_to_column("Variable")
p2 <- ggplot(imp2, aes(x = reorder(Variable, IncNodePurity), y = IncNodePurity)) +
geom_col() +
coord_flip() +
xlab("") +
theme_light()
# Importance: partial dependence
p3 <- vip(trn.rf, use.partial = TRUE, pred.var = paste0("x.", 1:10)) +
ylab("pdVarImp") +
theme_light()
# Display all three plots together
grid.arrange(p1, p2, p3, ncol = 3)
source('~/.active-rstudio-document', echo=TRUE)
getwd()
library(randomForest)
library(ggplot2)
library(pdp)
library(randomForest)
# Fit a random forest to the iris data
set.seed(101)  # for reproducibility
iris.rf <- randomForest(Species ~ ., data = iris)
plot(iris.rf)
# Load required packages
library(ggplot2)
library(pdp)
library(randomForest)
# Fit a random forest to the iris data
set.seed(101)  # for reproducibility
iris.rf <- randomForest(Species ~ ., data = iris)
# Partial dependence plot for Petal.Length on class 1 (i.e. setosa)
pd1 <- partial(iris.rf, pred.var = "Petal.Length", which.class = 1, prob = TRUE)
# Partial dependence plot for Petal.Length on class 2 (i.e. versicolor)
pd2 <- partial(iris.rf, pred.var = "Petal.Length", which.class = 2, prob = TRUE)
# Partial dependence plot for Petal.Length on class 3 (i.e. verginica)
pd3 <- partial(iris.rf, pred.var = "Petal.Length", which.class = 3, prob = TRUE)
# Bind together into a single data frame
pd <- rbind(cbind(pd1, "Class" = "1"),
cbind(pd2, "Class" = "2"),
cbind(pd3, "Class" = "3"))
# Display all three plots
ggplot(pd, aes(x = Petal.Length, y = yhat, color = Class)) +
geom_line() +
xlab("Petal length") +
ylab("Probability") +
theme_light()
library(vip)
?vip::gen_friedman
head(vip::gen_friedman())
trn <- vip::gen_friedman(500, seed = 101)  # simulate training data
names(trn) <- toupper(names(trn))  # to match notation
tibble::as_tibble(trn)  # inspect output
trn <- vip::gen_friedman(500, seed = 101)  # simulate training data
names(trn) <- toupper(names(trn))  # to match notation
tibble::as_tibble(trn)  # inspect output
trn <- vip::gen_friedman(500, seed = 101)  # simulate training data
names(trn) <- toupper(names(trn))  # to match notation
tibble::as_tibble(trn)  # inspect output
# Load required packages
library(nnet)
# Fit a neural network
set.seed(0803)
nn <- nnet(y ~ ., data = trn, size = 7, decay = 0.1, linout = TRUE)
# VIPs
p1 <- vip(nn, type = "garson")
p2 <- vip(nn, type = "olden")
# Figure 5
grid.arrange(p1, p2, nrow = 1)
library(vip)
# Load required packages
library(nnet)
# Fit a neural network
set.seed(0803)
nn <- nnet(y ~ ., data = trn, size = 7, decay = 0.1, linout = TRUE)
# VIPs
p1 <- vip(nn, type = "garson")
p2 <- vip(nn, type = "olden")
# Figure 5
grid.arrange(p1, p2, nrow = 1)
# Set global knitr chunk options
knitr::opts_chunk$set(
fig.width = 6,
fig.asp = 0.618,
out.width = "100%",
message = FALSE,
warning = FALSE
)
# Load required packages
library(dplyr)
library(vip)
trn <- vip::gen_friedman(500, sigma = 1, seed = 101)  # simulate training data
tibble::as_tibble(trn)  # inspect output
# Load required packages
library(nnet)
# Fit a neural network
set.seed(0803)
nn <- nnet(y ~ ., data = trn, size = 7, decay = 0.1, linout = TRUE)
# VIPs
p1 <- vip(nn, type = "garson")
p2 <- vip(nn, type = "olden")
# Figure 5
grid.arrange(p1, p2, nrow = 1)
# Set global knitr chunk options
knitr::opts_chunk$set(
fig.width = 6,
fig.asp = 0.618,
out.width = "70%",
fig.align = "center",
message = FALSE,
warning = FALSE
)
# Load required packages
library(dplyr)
library(vip)
# Construct PDP-based variable importance scores
(vis <- vi(pp, method = "pdp"))
# Fit a PPR model (nterms was chosen using the caret package with 5 repeats of
# 5-fold cross-validation)
pp <- ppr(y ~ ., data = trn, nterms = 11)
# Plot VI scores
p1 <- vip(pp, method = "pdp") + ggtitle("PPR")
# Construct PDP-based variable importance scores
(vis <- vi(pp, method = "pdp"))
# Reconstruct PDPs for all 10 features
par(mfrow = c(2, 5))
for (name in paste0("X", 1:10)) {
plot(attr(vis, which = "pdp")[[name]], type = "l", ylim = c(9, 19), las = 1)
}
# Construct PDP-based variable importance scores
(vis <- vi(pp, method = "pdp"))
# Reconstruct PDPs for all 10 features
par(mfrow = c(2, 5))
for (name in paste0("x", 1:10)) {
plot(attr(vis, which = "pdp")[[name]], type = "l", ylim = c(9, 19), las = 1)
}
package_version("vip")
packageVersion("vip")
library(dplyr)
library(vip)
trn <- vip::gen_friedman(500, sigma = 1, seed = 101)  # simulate training data
tibble::as_tibble(trn)  # inspect output
# Load required packages
library(rpart)          # for fitting CART-like decision trees
library(randomForest)   # for fitting RFs
library(xgboost)        # for fitting GBMs
# Fit a single regression tree
tree <- rpart(y ~ ., data = trn)
# Fit an RF
set.seed(101)  # for reproducibility
rfo <- randomForest(y ~ ., data = trn, importance = TRUE)
# Fit a GBM
set.seed(102)  # for reproducibility
bst <- xgboost(
data = data.matrix(subset(trn, select = -y)),
label = trn$y,
objective = "reg:squarederror",
nrounds = 100,
max_depth = 5,
eta = 0.3,
verbose = 0  # suppress printing
)
# Extract VI scores from each model
vi_tree <- tree$variable.importance
vi_rfo <- rfo$variable.importance  # or use `randomForest::importance(rfo)`
vi_bst <- xgb.importance(model = bst)
# VI plot for single regression tree
vi_tree <- tree$variable.importance %>%
data.frame("Importance" = .) %>%
tibble::rownames_to_column("Feature")
# VI plot for RF
vi_rfo <- rfo$importance %>%
data.frame("Importance" = .) %>%
tibble::rownames_to_column("Feature")
# VI plot for GMB
vi_bst <- bst %>%  #
xgb.importance(model = .) %>%
as.data.frame() %>%
select(Feature, Importance = Gain)
# Plot results
library(ggplot2)
p1 <- vip(tree) + ggtitle("Single tree")
p2 <- vip(rfo) + ggtitle("Random forest")
p3 <- vip(bst) + ggtitle("Gradient boosting")
grid.arrange(p1, p2, p3, nrow = 1)  # display plots in a grid
# Load required packages
library(vip)
# Compute model-specific VI scores
vi(tree)  # CART-like decision tree
vi(rfo)   # RF
vi(bst)   # GBM
p1 <- vip(tree) + ggtitle("Single tree")
p2 <- vip(rfo) + ggtitle("Random forest")
p3 <- vip(bst) + ggtitle("Gradient boosting")
# Display plots in a grid (Figure 1)
grid.arrange(p1, p2, p3, nrow = 1)
# Construct VIP (Figure 2)
library(ggplot2)  # for theme_light() function
vip(bst, num_features = 5, geom = "point", horizontal = FALSE,
aesthetics = list(color = "red", shape = 17, size = 5)) +
theme_light()
# Fit a LM
linmod <- lm(y ~ .^2, data = trn)
backward <- step(linmod, direction = "backward", trace = 0)
# Extract VI scores
(vi_backward <- vi(backward))
# Plot VI scores; by default, `vip()` displays the top ten features
pal <- palette.colors(2, palette = "Okabe-Ito")  # colorblind friendly
vip(vi_backward, num_features = length(coef(backward)),  # Figure 3
geom = "point", horizontal = FALSE, mapping = aes(color = Sign)) +
scale_color_manual(values = unname(pal)) +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Load required packages
library(earth)
# Fit a MARS model
mars <- earth(y ~ ., data = trn, degree = 2, pmethod = "exhaustive")
# Extract VI scores
vi(mars, type = "gcv")
# Plot VI scores (Figure 4)
vip(mars)
# Load required packages
library(nnet)
# Fit a neural network
set.seed(0803)  # for reproducibility
nn <- nnet(y ~ ., data = trn, size = 7, decay = 0.1,
linout = TRUE, trace = FALSE)
# Construct VIPs
p1 <- vip(nn, type = "garson")
p2 <- vip(nn, type = "olden")
# Display plots in a grid (Figure 5)
grid.arrange(p1, p2, nrow = 1)
# Load required packages
library(pdp)
# Fit a PPR model (nterms was chosen using the caret package with 5 repeats of
# 5-fold cross-validation)
pp <- ppr(y ~ ., data = trn, nterms = 11)
# PDPs for all 10 features
features <- paste0("x", 1:10)
pdps <- lapply(features, FUN = function(feature) {
pd <- partial(pp, pred.var = feature)
autoplot(pd) +
ylim(range(trn$y)) +
theme_light()
})
# Display plots in a grid
grid.arrange(grobs = pdps, ncol = 5)
# Fit a PPR model (nterms was chosen using the caret package with 5 repeats of
# 5-fold cross-validation)
pp <- ppr(y ~ ., data = trn, nterms = 11)
# Construct VIPs
p1 <- vip(pp, method = "firm") + ggtitle("PPR")
p2 <- vip(nn, method = "firm") + ggtitle("NN")
# Display plots in a grid (Figure 7)
grid.arrange(p1, p2, ncol = 2)
# ICE curves for all 10 features
ice_curves <- lapply(features, FUN = function(feature) {
ice <- partial(pp, pred.var = feature, ice = TRUE)
autoplot(ice, alpha = 0.1) +
ylim(range(trn$y)) +
theme_light()
})
# Display plots in a grid (Figure 8)
grid.arrange(grobs = ice_curves, ncol = 5)
# Construct VIPs
p1 <- vip(pp, method = "firm", ice = TRUE) + ggtitle("PPR")
p2 <- vip(nn, method = "firm", ice = TRUE) + ggtitle("NN")
# Display plots in a grid (Figure 9)
grid.arrange(p1, p2, ncol = 2)
# Construct PDP-based VI scores
(vis <- vi(pp, method = "firm"))
# Reconstruct PDPs for all 10 features (Figure 10)
par(mfrow = c(2, 5))
for (name in paste0("x", 1:10)) {
plot(attr(vis, which = "effects")[[name]], type = "l", ylim = c(9, 19), las = 1)
}
# Plot VI scores
set.seed(2021)  # for reproducibility
p1 <- vip(pp, method = "permute", target = "y", metric = "rsquared",
pred_wrapper = predict) + ggtitle("PPR")
p2 <- vip(nn, method = "permute", target = "y", metric = "rsquared",
pred_wrapper = predict) + ggtitle("NN")
# Display plots in a grid (Figure 11)
grid.arrange(p1, p2, ncol = 2)
# Use 10 Monte Carlo reps
set.seed(403)  # for reproducibility
vis <- vi(pp, method = "permute", target = "y", metric = "rsquared",
pred_wrapper = predict, nsim = 15)
vip(vis, geom = "boxplot")  # Figure 12
list_metrics()
mae <- function(actual, predicted) {
mean(abs(actual - predicted))
}
# Construct VIP (Figure 13)
set.seed(2321)  # for reproducibility
pfun <- function(object, newdata)  predict(object, newdata = newdata)
vip(nn, method = "permute", target = "y", metric = mae,
smaller_is_better = TRUE, pred_wrapper = pfun) +
ggtitle("Custom loss function: MAE")
# Construct VIP (Figure 14)
set.seed(2327)  # for reproducibility
vip(nn, method = "permute", pred_wrapper = pfun, target = "y", metric = "rmse",
train = trn[sample(nrow(trn), size = 400), ]) +  # sample 400 observations
ggtitle("Using a random subset of training data")
# Construct VIP (Figure 15)
set.seed(8264)  # for reproducibility
vip(nn, method = "permute", pred_wrapper = pfun, target = "y", metric = "mae",
nsim = 10, geom = "point", all_permutations = TRUE, jitter = TRUE) +
ggtitle("Plotting all permutation scores")
# Load required packages
library(microbenchmark)
mb <- readRDS("benchmark.rds")
autoplot(mb)  # Figure 16
# Load required packages
library(xgboost)
# Feature matrix
X <- data.matrix(subset(trn, select = -y))  # matrix of feature values
# Fit an XGBoost model; hyperparameters were tuned using 5-fold CV
set.seed(859)  # for reproducibility
bst <- xgboost(X, label = trn$y, nrounds = 338, max_depth = 3, eta = 0.1,
verbose = 0)
# Construct VIP (Figure 17)
vip(bst, method = "shap", train = X, exact = TRUE, include_type = TRUE)
# Load required packages
library(mlr3)
library(mlr3learners)
# Fit a ranger-based random forest using the mlr3 package
set.seed(101)
task <- TaskRegr$new("friedman", backend = trn, target = "y")
lrnr <- lrn("regr.ranger", importance = "impurity")
lrnr$train(task)
# First, compute a tibble of VI scores using any method
var_imp <- vi(lrnr)
# Next, convert to an HTML-based data table with sparklines
add_sparklines(var_imp, fit = lrnr$model, train = trn)  # Figure 18
install.packages("sparkline")
# Load required packages
library(mlr3)
library(mlr3learners)
# Fit a ranger-based random forest using the mlr3 package
set.seed(101)
task <- TaskRegr$new("friedman", backend = trn, target = "y")
lrnr <- lrn("regr.ranger", importance = "impurity")
lrnr$train(task)
# First, compute a tibble of VI scores using any method
var_imp <- vi(lrnr)
# Next, convert to an HTML-based data table with sparklines
add_sparklines(var_imp, fit = lrnr$model, train = trn)  # Figure 18
library(doParallel) # load the parallel backend
cl <- makeCluster(5) # use 5 workers
registerDoParallel(cl) # register the parallel backend
pal <- palette.colors(2, palette = "Okabe-Ito")  # colorblind friendly
vip(vi_backward, num_features = length(coef(backward)),  # Figure 3
geom = "point", horizontal = FALSE, mapping = aes(color = Sign)) +
scale_color_manual(values = unname(pal)) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme_light()
install.packages(c("ingredients", "mmpf", "iml"))
add_sparklines
vip:::add_sparklines.vi
install.packages("webshot")
webshot::install_phantomjs()
?palette
# Fit a LM
linmod <- lm(y ~ .^2, data = trn)
backward <- step(linmod, direction = "backward", trace = 0)
# Extract VI scores
(vi_backward <- vi(backward))
# Plot VI scores; by default, `vip()` displays the top ten features
pal <- palette.colors(2, palette = "Okabe-Ito")  # colorblind friendly palette
vip(vi_backward, num_features = length(coef(backward)),  # Figure 3
geom = "point", horizontal = FALSE, mapping = aes(color = Sign)) +
scale_color_manual(values = unname(pal)) +
theme_light() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
fun1 <- function(m, p, column = 3) {
fastshap:::genOMat2(m, p, column)
}
fun2 <- function(m, p, column = 3) {
pos <- sample(p - 1, size = m, replace = TRUE)
O <- t(sapply(pos, FUN = function(x) {
sample(c(TRUE, FALSE), size = p - 1, replace = TRUE,
prob = c(x / (p - 1), 1 - x / (p - 1)))
}))
O <- if (column == 1) {  # case 1
cbind(TRUE, O)
} else if (column == p) {  # case 2
cbind(O, TRUE)
} else {  # case 3
cbind(
O[, 1:(column - 1), drop = FALSE],
TRUE,
O[, column:(p - 1), drop = FALSE]
)
}
O
}
mb <- microbenchmark(
fun1(10000, 25),
fun2(10000, 25),
times = 1000
)
fun1 <- function(m, p, column = 3) {
O <- fastshap:::genOMat(m, p)
O <- if (column == 1) {  # case 1
cbind(TRUE, O)
} else if (column == p) {  # case 2
cbind(O, TRUE)
} else {  # case 3
cbind(
O[, 1:(column - 1), drop = FALSE],
TRUE,
O[, column:(p - 1), drop = FALSE]
)
}
O
}
fun2 <- function(m, p, column = 3) {
pos <- sample(p - 1, size = m, replace = TRUE)
O <- t(sapply(pos, FUN = function(x) {
sample(c(TRUE, FALSE), size = p - 1, replace = TRUE,
prob = c(x / (p - 1), 1 - x / (p - 1)))
}))
O <- if (column == 1) {  # case 1
cbind(TRUE, O)
} else if (column == p) {  # case 2
cbind(O, TRUE)
} else {  # case 3
cbind(
O[, 1:(column - 1), drop = FALSE],
TRUE,
O[, column:(p - 1), drop = FALSE]
)
}
O
}
mb <- microbenchmark(
fun1(10000, 25),
fun2(10000, 25),
times = 1000
)
fun2 <- function(m, p, column = 3) {
pos <- sample(p - 1, size = m, replace = TRUE)
t(sapply(pos, FUN = function(x) {
sample(c(TRUE, FALSE), size = p - 1, replace = TRUE,
prob = c(x / (p - 1), 1 - x / (p - 1)))
}))
}
proportions(table(rowSums(fun2(10000, 20))))
proportions(table(rowSums(fun2(10000, 20))))
proportions(table(rowSums(fun2(10000, 20))))
proportions(table(rowSums(fun1(10000, 20))))
fun1 <- function(m, p) {
fastshap:::genOMat(m, p)
}
fun2 <- function(m, p) {
pos <- sample(p - 1, size = m, replace = TRUE)
t(sapply(pos, FUN = function(x) {
sample(c(TRUE, FALSE), size = p - 1, replace = TRUE,
prob = c(x / (p - 1), 1 - x / (p - 1)))
}))
}
mb <- microbenchmark(
fun1(10000, 25),
fun2(10000, 25),
times = 1000
)
mb <- microbenchmark(
fun1(10000, 25),
fun2(10000, 25),
times = 10
)
autoplot(mb)
mb <- microbenchmark(
fun1(10000, 25),
fun2(10000, 25),
times = 100
)
autoplot(mb)
mb
65  5
65 / 5
