% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/vi_model.R
\name{vi_model}
\alias{vi_model}
\alias{vi_model.default}
\alias{vi_model.C5.0}
\alias{vi_model.constparty}
\alias{vi_model.earth}
\alias{vi_model.gbm}
\alias{vi_model.H2OBinomialModel}
\alias{vi_model.H2OMultinomialModel}
\alias{vi_model.H2ORegressionModel}
\alias{vi_model.lm}
\alias{vi_model.ml_model_decision_tree_regression}
\alias{vi_model.ml_model_decision_tree_classification}
\alias{vi_model.ml_model_gbt_regression}
\alias{vi_model.ml_model_gbt_classification}
\alias{vi_model.ml_model_generalized_linear_regression}
\alias{vi_model.ml_model_linear_regression}
\alias{vi_model.ml_model_random_forest_regression}
\alias{vi_model.ml_model_random_forest_classification}
\alias{vi_model.randomForest}
\alias{vi_model.RandomForest}
\alias{vi_model.ranger}
\alias{vi_model.rpart}
\alias{vi_model.train}
\alias{vi_model.xgb.Booster}
\title{Model-Based Variable Importance}
\usage{
vi_model(object, ...)

\method{vi_model}{default}(object, ...)

\method{vi_model}{C5.0}(object, ...)

\method{vi_model}{constparty}(object, ...)

\method{vi_model}{earth}(object, ...)

\method{vi_model}{gbm}(object, ...)

\method{vi_model}{H2OBinomialModel}(object, ...)

\method{vi_model}{H2OMultinomialModel}(object, ...)

\method{vi_model}{H2ORegressionModel}(object, ...)

\method{vi_model}{lm}(object, ...)

\method{vi_model}{ml_model_decision_tree_regression}(object, ...)

\method{vi_model}{ml_model_decision_tree_classification}(object, ...)

\method{vi_model}{ml_model_gbt_regression}(object, ...)

\method{vi_model}{ml_model_gbt_classification}(object, ...)

\method{vi_model}{ml_model_generalized_linear_regression}(object, ...)

\method{vi_model}{ml_model_linear_regression}(object, ...)

\method{vi_model}{ml_model_random_forest_regression}(object, ...)

\method{vi_model}{ml_model_random_forest_classification}(object, ...)

\method{vi_model}{randomForest}(object, ...)

\method{vi_model}{RandomForest}(object, auc = FALSE, ...)

\method{vi_model}{ranger}(object, ...)

\method{vi_model}{rpart}(object, ...)

\method{vi_model}{train}(object, ...)

\method{vi_model}{xgb.Booster}(object, type = NULL, ...)
}
\arguments{
\item{object}{A fitted model object (e.g., a \code{"randomForest"} object).}

\item{...}{Additional optional arguments.}

\item{auc}{Logical indicating whether or not to compute the AUC-based
variable scores described in Janitza et al. (2012). Only available for
\code{\link[party]{cforest}} objects. See
\code{\link[party]{varimpAUC}} for details. Default is \code{FALSE}.}

\item{type}{For xgboost models, the type of variable importance to return.
If \code{NULL} (the default), "Gain" is used. See
\code{\link[xgboost]{xgb.importance}} for details.}
}
\value{
A tidy data frame (i.e., a \code{"tibble"} object) with two columns:
\code{Variable} and \code{Importance}. For \code{"lm"/"glm"}-like object, an
additional column, called \code{Sign}, is also included which includes the
sign (i.e., POS/NEG) of the original coefficient.
}
\description{
Compute model-based variable importance scores for the predictors in a model.
(This function is meant for internal use only.)
}
\details{
Computes model-based variable importance scores depending on the class of
\code{object}:

\describe{

\item{\code{\link[C50]{C5.0}}}{Variable importance is measured by determining
the percentage of training set samples that fall into all the terminal nodes
after the split. For example, the predictor in the first split automatically
has an importance measurement of 100 percent since all samples are affected
by this split. Other predictors may be used frequently in splits, but if the
terminal nodes cover only a handful of training set samples, the importance
scores may be close to zero. The same strategy is applied to rule-based
models and boosted versions of the model. The underlying function can also
return the number of times each predictor was involved in a split by using
the option \code{metric = "usage"}. See \code{\link[C50]{C5imp}} for
details.}

\item{\code{\link[partykit]{cforest}}}{Variable importance is measured in a
way similar to those computed by \code{\link[randomForest]{importance}}.
Besides the standard version, a conditional version is available that
adjusts for correlations between predictor variables. If
\code{conditional = TRUE}, the importance of each variable is computed by
permuting within a grid defined by the predictors that are associated (with
1 - \emph{p}-value greater than threshold) to the variable of interest. The
resulting variable importance score is conditional in the sense of beta
coefficients in regression models, but represents the effect of a variable in
both main effects and interactions. See Strobl et al. (2008) for details.
Note, however, that all random forest results are subject to random
variation. Thus, before interpreting the importance ranking, check whether
the same ranking is achieved with a different random seed - or otherwise
increase the number of trees ntree in \code{\link[partykit]{ctree_control}}.
Note that in the presence of missings in the predictor variables the
procedure described in Hapfelmeier et al. (2012) is performed. See
\code{\link[partykit]{varimp}} for details.}

\item{\code{\link[earth]{earth}}}{The variable importance method for
\code{\link[earth]{earth}} objects first calculates the decrease in the GCV for each
subset relative to the previous subset during earthâ€™s pruning pass. (For
multiple response models, GCV's are calculated over all responses.) Then, for
each variable, it sums these decreases over all subsets that include the
variable. Finally, for ease of interpretation, the summed decreases are
scaled so the largest summed decrease is 100. Variables which cause larger
net decreases in the GCV are considered more important. See
\code{\link[earth]{evimp}} for details.}

\item{\code{\link[gbm]{gbm}}}{Variable importance is computed according to
Friedman (2001). For tree-based methods, the approximate relative influence
of a variable \eqn{x_j} is \deqn{\widehat{J}_j^2 = \sum{\text{splits on } x_j}I_t^2}
where \eqn{I_t} is the empirical improvement by splitting on \eqn{x_j} at that point.
Friedman's extension to boosted models is to average the relative influence
of variable \eqn{x_j} across all the trees generated by the boosting algorithm.
See \code{\link[gbm]{summary.gbm}} for details.}

\item{\code{\link[h2o]{H2OModel}}}{See \code{\link[h2o]{h2o.varimp}} or visit
\url{http://docs.h2o.ai/h2o/latest-stable/h2o-docs/variable-importance.html}
for details.}

\item{\code{\link[stats]{lm}}}{In (generalized) linear models, variable
importance is based on the absolute value of the corresponding
\emph{t}-statistics. For such models, the sign of the original coefficient
is also returned.}

\item{\code{\link[sparklyr::ml_feature_importances]{ml_model}}}{The Spark ML
library provides standard variable importance for tree-based methods (e.g.,
random forests). See \code{\link[sparklyr]{ml_feature_importances}} for
details.}

\item{\code{\link[randomForest]{randomForest}}}{Random forests typically
provide two measures of variable importance. The first measure is computed
from permuting out-of-bag (OOB) data: for each tree, the prediction error on
the OOB portion of the data is recorded (error rate for classification and
MSE for regression). Then the same is done after permuting each predictor
variable. The difference between the two are then averaged over all trees in
the forest, and normalized by the standard deviation of the differences. If
the standard deviation of the differences is equal to 0 for a variable,
the division is not done (but the average is almost always equal to 0 in that
case).

The second measure is the total decrease in node impurities from splitting on
the variable, averaged over all trees. For classification, the node impurity
is measured by the Gini index. For regression, it is measured by residual sum
of squares. See \code{\link[randomForest]{importance}} for details.}

\item{\code{\link[party]{RandomForest}}}{Same approach described in
\code{\link[partykit]{cforest}} above. See \code{\link[party]{varimp}} and
\code{\link[party]{varimpAUC}} (if \code{auc = TRUE}) for details.}

\item{\code{\link[ranger]{ranger}}}{Variable importance for
\code{\link[ranger]{ranger}} objects is computed in the usual way for random
forests. The approach used depends on the \code{importance} argument provided
in the initial call to \code{\link[ranger]{ranger}}. See
\code{\link[ranger]{importance}} for details.}

\item{\code{\link[rpart]{rpart}}}{As stated in one of the \strong{rpart}
vignettes. A variable may appear in the tree many times, either as a primary
or a surrogate variable. An overall measure of variable importance is the sum
of the goodness of split measures for each split for which it was the primary
variable, plus "goodness" * (adjusted agreement) for all splits in which it
was a surrogate. Imagine two variables which were essentially duplicates of
each other; if we did not count surrogates, they would split the importance
with neither showing up as strongly as it should. See
\code{\link[rpart]{rpart}} for details.}

\item{\code{\link[caret]{train}}}{Various model-specific and model-agnostic
approaches that depend on the learning algorithm employed in the original
call to \code{\link[caret]{train}}. See \code{\link[caret]{varImp}} for
details.}

\item{\code{\link[xgboost]{xgb.Booster}}}{For linear models, the variable
importance is the absolute magnitude of the estimated coefficients. For that
reason, in order to obtain a meaningful ranking by importance for a linear
model, the features need to be on the same scale (which you also would want
to do when using either L1 or L2 regularization). Otherwise, the approach
described in Friedman (2001) for \code{\link[gbm]{gbm}}s is used. See
\code{\link[xgboost]{xgb.importance}} for details.}

}
}
\note{
Inspired by the \code{\link[caret]{varImp}} function.
}
