---
output:
  md_document:
    variant: markdown_github
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "tools/README-"
)
```

# vip: Variable Importance Plots <img src="tools/vip-logo.png" align="right" />

Variable importance plots.


## Installation

The `vip` package is currently only available from GitHub, but can easily be installed using the [devtools](https://CRAN.R-project.org/package=devtools) package:
```{r, eval=FALSE}
if (!requireNamespace("devtools")) install.packages("devtools")
devtools::install_github("AFIT-R/vip")
```


## Example usage

For illustration, we use one of the regression problems described in Friedman (1991) and Breiman (1996). Inputs are 10 independent variables uniformly distributed on the interval $\left[0, 1\right]$; only 5 out of these 10 are actually used. Outputs are created according to the formula

$$
\mathcal{Y} = 10 \sin\left(\pi x_1 x_2\right) + 20 \left(x_3 - 0.5\right) ^ 2 + 10 x_4 + 5 x_5 + \epsilon,
$$

where $\epsilon \sim N\left(0, \sigma\right)$. These data are available in the [mlbench](https://CRAN.R-project.org/package=mlbench) package. The code chunk below simulates 500 observations from the above model with $\simga = 1$.
```{r}
# Load required packages
library(mlbench)

# Simulate training data
set.seed(101)  # for reproducibility
trn <- as.data.frame(mlbench.friedman1(500))  # ?mlbench.friedman1
tibble::glimpse(trn)  # take a peek
```

Next, we fit a random forest to the simulated data and construct variable importance plots using the two methods provided by the random forest algorithm (left and middle plots) and the partial dependence approach (right plot).
```{r example-rf, message=FALSE, warning=FALSE, fig.width=10, fig.height=7}
# Load required packages
library(ggplot2)
library(magrittr)
library(randomForest)  
library(vip)

# Fit a random forest
set.seed(102)
trn.rf <- randomForest(y ~ ., data = trn, importance = TRUE)

# Importance: mean decrease in accuracy
imp1 <- importance(trn.rf, type = 1) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Variable")
p1 <- ggplot(imp1, aes(x = reorder(Variable, `%IncMSE`), y = `%IncMSE`)) +
  geom_col() +
  coord_flip() +
  xlab("") +
  theme_light()

# Importance: mean decrease node impurity
imp2 <- importance(trn.rf, type = 2) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Variable")
p2 <- ggplot(imp2, aes(x = reorder(Variable, IncNodePurity), y = IncNodePurity)) +
  geom_col() +
  coord_flip() +
  xlab("") +
  theme_light()

# Importance: partial dependence
p3 <- vip(trn.rf, use.partial = TRUE, pred.var = paste0("x.", 1:10)) +
  ylab("pdVarImp") +
  theme_light()

# Display all three plots together
grid.arrange(p1, p2, p3, ncol = 3)
```
