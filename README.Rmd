---
output:
  md_document:
    variant: markdown_github
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "tools/README-"
)
```

# vip: Variable Importance Plots <img src="tools/vip-logo.png" align="right" width="120" height="139" />

Variable importance plots.


## Installation

The `vip` package is currently only available from GitHub, but can easily be installed using the [devtools](https://CRAN.R-project.org/package=devtools) package:
```{r, eval=FALSE}
if (!requireNamespace("devtools")) install.packages("devtools")
devtools::install_github("AFIT-R/vip")
```


## Example usage

For illustration, we use one of the regression problems described in Friedman (1991) and Breiman (1996). Inputs are 10 independent variables uniformly distributed on the interval $\left[0, 1\right]$; only 5 out of these 10 are actually used. Outputs are created according to the formula

$$
\mathcal{Y} = 10 \sin\left(\pi x_1 x_2\right) + 20 \left(x_3 - 0.5\right) ^ 2 + 10 x_4 + 5 x_5 + \epsilon,
$$

where $\epsilon \sim \mathcal{N}\left(0, \sigma\right)$. These data are available in the [mlbench](https://CRAN.R-project.org/package=mlbench) package. The code chunk below simulate 500 observations from the above model with $\simga = 1$.
```{r}
if (!requireNamespace("mlbench")) install.packages("mlbench")
set.seed(101)  # for reproducibility
trn <- as.data.frame(mlbench::mlbench.friedman1(n = 500, sd = 1))
tibble::glimpse(trn)
```


### Linear model

```{r example-lm, fig.width=8, fig.height=4, message=FALSE}
library(vip)
trn.lm <- lm(y ~ ., data = trn)
p1 <- vip(trn.lm)
p2 <- vip(trn.lm, use.partial = TRUE)
grid.arrange(p1, p2, ncol = 2)  # display plots side-by-side
```


### Random forest

```{r example-rf}
library(randomForest)  # install.packages("randomForest")
set.seed(102)
trn.rf <- randomForest(y ~ ., data = trn, importance = TRUE)
importance(trn.rf)  # for comparison
vip(trn.rf, use.partial = TRUE, pred.var = paste0("x.", 1:10))
```


### Neural network

```{r example-nn}
library(ggplot2)
library(nnet)  # install.packages("nnet")
set.seed(103)
trn.nn <- nnet(y ~ ., data = trn, size = 10, linout = TRUE, decay = 0.001,
               maxit = 1000, trace = FALSE)
vip(trn.nn, use.partial = TRUE, pred.var = paste0("x.", 1:10), alpha = 0.5) +
  theme_light() +
  ylab("Partial dependence-based variable importance") +
  ggtitle("Neural network variable importance scores")
```

You can also request the partial dependence data be returned in an attribute called `"partial"`. For example, we can see that the fitted neural network did indeed pick up the quadratic relationship between $x_3$ and $\mathcal{Y}$:
```{r, exampe-nn-2}
pdVarImp(trn.nn, pred.var = "x.3")  # same value displayed in above plot
imp <- pdVarImp(trn.nn, pred.var = "x.3", return.partial = TRUE)
ggplot2::autoplot(attr(imp, "partial"))
```
